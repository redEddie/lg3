{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오성사 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import io\n",
    "\n",
    "# ============================================================\n",
    "# 0) 기본 설정 (1시간 기준) - 단일 구간으로 통합\n",
    "# ============================================================\n",
    "SMARTCARE_DIR = Path(\"/home/hd/Desktop/LOG_SMARTCARE_20250930_OS\")\n",
    "EREPORT_DIR   = Path(\"/home/hd/Desktop/LOG_EREPORT_OS\")\n",
    "WEATHER_CSV   = Path(\"/home/hd/Desktop/LG/OBS_ASOS_TIM_20250701_20250930_OS.csv\")\n",
    "\n",
    "OUT_CSV = \"./preprocessed_1h_master_with_weather_delta_20250701_20250930_OS.csv\"\n",
    "\n",
    "start = pd.Timestamp(\"2025-07-01 00:00:00\")\n",
    "end   = pd.Timestamp(\"2025-09-30 23:00:00\")\n",
    "master_index = pd.date_range(start=start, end=end, freq=\"1H\")\n",
    "\n",
    "# 사용자 정의 휴일(날짜만)\n",
    "HOLIDAY_DATES = pd.to_datetime([\n",
    "    # 7~8월\n",
    "    \"2025-07-05\",\n",
    "    \"2025-07-06\", \"2025-07-12\",\n",
    "    \"2025-07-13\", \"2025-07-19\",\n",
    "    \"2025-07-20\",\n",
    "    \"2025-07-26\", \"2025-07-27\",\n",
    "    \"2025-08-02\", \"2025-08-03\",\n",
    "    \"2025-08-09\", \"2025-08-10\",\n",
    "\n",
    "    # 8/12 결측 이벤트를 \"주말/휴일\"로 마크하고 싶다면 날짜 추가\n",
    "    \"2025-08-12\",\n",
    "\n",
    "    # 8~9월\n",
    "    \"2025-08-15\", \"2025-08-16\", \"2025-08-17\",\n",
    "    \"2025-08-23\", \"2025-08-24\", \"2025-08-30\", \"2025-08-31\",\n",
    "    \"2025-09-06\", \"2025-09-07\", \"2025-09-13\", \"2025-09-14\",\n",
    "    \"2025-09-20\", \"2025-09-21\", \"2025-09-27\", \"2025-09-28\"\n",
    "]).date\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Coverage / Smartcare coverage\n",
    "# ------------------------------------------------------------\n",
    "POWER_COVERAGE_THRESHOLD = 0.85\n",
    "SMART_PER_UNIT_MIN_RATIO = 0.70\n",
    "N_UNITS = 7\n",
    "SMART_SAMPLE_SEC = 5\n",
    "\n",
    "# ---- Fourier(자정 롤링) 설정\n",
    "FOURIER_S = 168.0\n",
    "FOURIER_K = 10\n",
    "FOURIER_FIT_HOURS = 14 * 24          # 2 weeks = 336\n",
    "FOURIER_RIDGE_L2 = 1e-2\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# \"강제로 OFF(전력 0)로 처리\"할 구간 지정 (ceil 기준 일관성)\n",
    "# - 당신의 의도: 8/11 21시 이후(= 22시부터) + 8/12 전체 OFF\n",
    "# - ceil(\"1h\")이면 8/12 23시(이전 1시간)가 8/13 00시 slot로 귀속됨\n",
    "#   => OFF 강제 범위 끝을 8/13 00:00까지 포함해야 함\n",
    "# ------------------------------------------------------------\n",
    "FORCED_OFF_RANGES = [\n",
    "    (\"2025-08-11 22:00:00\", \"2025-08-13 00:00:00\"),\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 1) 파일 이름에서 날짜(YYYYMMDD) 뽑기\n",
    "# ============================================================\n",
    "def extract_date_from_filename(filename: str) -> pd.Timestamp:\n",
    "    m = re.search(r\"(\\d{8})\", filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"파일명에서 날짜(YYYYMMDD)를 찾을 수 없습니다: {filename}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Time 컬럼과 파일명 날짜로 datetime 만들기\n",
    "# ============================================================\n",
    "def make_datetime_from_time_and_filename(df: pd.DataFrame, file_path: Path) -> pd.DataFrame:\n",
    "    if \"Time\" not in df.columns:\n",
    "        raise ValueError(f\"'Time' 컬럼이 없습니다. 파일: {file_path}\")\n",
    "\n",
    "    file_date = extract_date_from_filename(file_path.name)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        file_date.strftime(\"%Y-%m-%d\") + \" \" + df[\"Time\"].astype(str),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df = df.dropna(subset=[\"datetime\"])\n",
    "    df = df.sort_values(\"datetime\").set_index(\"datetime\")\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3) NULL(\\x00) 제거 후 read_csv\n",
    "# ============================================================\n",
    "def read_csv_remove_nulls(path: Path) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as fh:\n",
    "        raw = fh.read()\n",
    "    if b\"\\x00\" in raw:\n",
    "        print(f\"[WARN] NULL 바이트 감지 → 제거 후 로드: {path.name}\")\n",
    "        raw = raw.replace(b\"\\x00\", b\"\")\n",
    "    return pd.read_csv(io.BytesIO(raw))\n",
    "\n",
    "# ============================================================\n",
    "# 4) 폴더 안 CSV를 모두 읽어서 concat\n",
    "# ============================================================\n",
    "def load_all_csv_time_from_filename(folder: Path, pattern: str) -> pd.DataFrame | None:\n",
    "    files = sorted(folder.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[WARN] {folder} 에 {pattern}에 해당하는 파일이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df_raw = read_csv_remove_nulls(f)\n",
    "            df = make_datetime_from_time_and_filename(df_raw, f)\n",
    "            df[\"src_file\"] = f.name\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f} 로드 실패: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return None\n",
    "\n",
    "    return pd.concat(dfs, axis=0).sort_index()\n",
    "\n",
    "# ============================================================\n",
    "# 5) 시간 feature 생성 (1시간 기준)\n",
    "#    - 요구: \"주말 마크\" -> 토/일도 is_holiday=1로 포함\n",
    "# ============================================================\n",
    "def create_time_features_1h(index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    df_time = pd.DataFrame(index=index)\n",
    "\n",
    "    dates = df_time.index.date\n",
    "    is_custom_holiday = pd.Series(dates).isin(set(HOLIDAY_DATES)).to_numpy()\n",
    "\n",
    "    dow = df_time.index.dayofweek.to_numpy()  # Mon=0 .. Sun=6\n",
    "    is_weekend = (dow >= 5)\n",
    "\n",
    "    df_time[\"is_holiday\"] = (is_custom_holiday | is_weekend).astype(int)\n",
    "\n",
    "    hour_of_day = df_time.index.hour.astype(int)\n",
    "    df_time[\"day_sin\"] = np.sin(2 * np.pi * hour_of_day / 24.0)\n",
    "    df_time[\"day_cos\"] = np.cos(2 * np.pi * hour_of_day / 24.0)\n",
    "\n",
    "    # week cycle: master_index 시작을 0으로 둔 위상\n",
    "    k = np.arange(len(df_time), dtype=float)\n",
    "    df_time[\"week_sin\"] = np.sin(2 * np.pi * k / 168.0)\n",
    "    df_time[\"week_cos\"] = np.cos(2 * np.pi * k / 168.0)\n",
    "\n",
    "    return df_time\n",
    "\n",
    "# ============================================================\n",
    "# 6) EREPORT 전처리 (1시간 Power 합산 + 결측 보정) + 강제 OFF(0)\n",
    "#    - slot = ceil(\"1h\") 유지 (이전 1시간을 다음 정각 slot로 귀속)\n",
    "# ============================================================\n",
    "def preprocess_ereport_power_1h(\n",
    "    df_ereport: pd.DataFrame,\n",
    "    master_index: pd.DatetimeIndex,\n",
    "    freq: str = \"1h\",\n",
    "    coverage_threshold: float = 0.85,\n",
    "    forced_zero_ranges: list[tuple[str | pd.Timestamp, str | pd.Timestamp]] | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    POWER_COL = \"Power\"\n",
    "    if POWER_COL not in df_ereport.columns:\n",
    "        raise ValueError(f\"EREPORT에 '{POWER_COL}' 컬럼이 없습니다. 현재 컬럼: {df_ereport.columns.tolist()}\")\n",
    "\n",
    "    df = df_ereport.copy()\n",
    "    df[\"slot\"] = df.index.ceil(freq)  # ✅ 기준 유지\n",
    "\n",
    "    diffs = df.index.to_series().diff().dropna().dt.total_seconds()\n",
    "    if len(diffs) == 0:\n",
    "        raise RuntimeError(\"EREPORT 로그에서 시간 간격을 추정할 수 없습니다.\")\n",
    "    median_step = diffs.median()\n",
    "    if median_step <= 0 or np.isnan(median_step):\n",
    "        median_step = 60.0\n",
    "\n",
    "    expected_rows = int(round(3600 / median_step))\n",
    "    expected_rows = max(expected_rows, 1)\n",
    "    min_required_rows = int(expected_rows * coverage_threshold)\n",
    "\n",
    "    grp_sum = df.groupby(\"slot\")[POWER_COL].sum()\n",
    "    grp_n   = df.groupby(\"slot\")[POWER_COL].size()\n",
    "\n",
    "    out = pd.DataFrame(index=grp_sum.index)\n",
    "    out[\"Power_1h_rawsum\"] = grp_sum\n",
    "    out[\"n_rows\"] = grp_n\n",
    "    out[\"Power_1h\"] = np.nan\n",
    "\n",
    "    for slot, row in out.iterrows():\n",
    "        n = int(row[\"n_rows\"])\n",
    "        if n < min_required_rows:\n",
    "            continue\n",
    "\n",
    "        P_obs = float(row[\"Power_1h_rawsum\"])\n",
    "        m = expected_rows - n\n",
    "        if m <= 0:\n",
    "            out.at[slot, \"Power_1h\"] = P_obs\n",
    "            continue\n",
    "\n",
    "        sub = df[df[\"slot\"] == slot][[POWER_COL]].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        # ceil 기준 윈도우는 (slot-1h, slot]\n",
    "        w_end = pd.Timestamp(slot)\n",
    "        w_start = w_end - pd.Timedelta(freq)\n",
    "\n",
    "        step = pd.Timedelta(seconds=float(median_step))\n",
    "        grid = pd.date_range(start=w_start + step, end=w_end, freq=step)\n",
    "\n",
    "        obs_set = set(sub.index)\n",
    "        missing_times = [t for t in grid if t not in obs_set]\n",
    "        m_cnt = m\n",
    "\n",
    "        obs_series = sub[POWER_COL].sort_index()\n",
    "        past_nonzero = False\n",
    "        future_nonzero = False\n",
    "\n",
    "        probe = missing_times\n",
    "        if len(probe) > 10:\n",
    "            probe = probe[:: max(1, len(probe)//10)]\n",
    "\n",
    "        for tmiss in probe:\n",
    "            past = obs_series.loc[:tmiss]\n",
    "            if len(past) > 0 and float(past.iloc[-1]) != 0.0:\n",
    "                past_nonzero = True\n",
    "\n",
    "            fut = obs_series.loc[tmiss:]\n",
    "            if len(fut) > 0 and float(fut.iloc[0]) != 0.0:\n",
    "                future_nonzero = True\n",
    "\n",
    "            if past_nonzero and future_nonzero:\n",
    "                break\n",
    "\n",
    "        if past_nonzero and future_nonzero:\n",
    "            P_adj = P_obs * (expected_rows / n)\n",
    "        elif past_nonzero ^ future_nonzero:\n",
    "            avg_obs = P_obs / max(n, 1)\n",
    "            P_adj = P_obs + 0.5 * avg_obs * m_cnt\n",
    "        else:\n",
    "            P_adj = P_obs\n",
    "\n",
    "        out.at[slot, \"Power_1h\"] = P_adj\n",
    "\n",
    "    # =========================================================\n",
    "    # ✅ 핵심: 먼저 master_index로 reindex해서 \"빈 slot 행\"을 생성\n",
    "    # =========================================================\n",
    "    out = out.reindex(master_index)\n",
    "\n",
    "    # =========================================================\n",
    "    # ✅ 그 다음 강제 OFF를 적용해야 실제로 NaN이 0으로 덮임\n",
    "    # =========================================================\n",
    "    if forced_zero_ranges:\n",
    "        for a, b in forced_zero_ranges:\n",
    "            a = pd.Timestamp(a)\n",
    "            b = pd.Timestamp(b)\n",
    "            mask = (out.index >= a) & (out.index <= b)\n",
    "            out.loc[mask, \"Power_1h\"] = 0.0\n",
    "\n",
    "    return out[[\"Power_1h\"]]\n",
    "\n",
    "# ============================================================\n",
    "# 7) SMARTCARE 전처리 (1시간 Tod)\n",
    "#    - 기존과 동일 (ceil 유지)\n",
    "# ============================================================\n",
    "def preprocess_smartcare_tod_1h(\n",
    "    df_smart: pd.DataFrame,\n",
    "    master_index: pd.DatetimeIndex,\n",
    "    n_units: int = 7,\n",
    "    sample_interval_sec: int = 5,\n",
    "    freq: str = \"1h\",\n",
    "    per_unit_min_ratio: float = 0.70\n",
    ") -> pd.DataFrame:\n",
    "    REQUIRED_COLS = [\"Auto Id\", \"Tod\"]\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df_smart.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"SMARTCARE에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "    df = df_smart.copy()\n",
    "    df[\"slot\"] = df.index.ceil(freq)\n",
    "\n",
    "    expected_per_unit = int(round(3600 / sample_interval_sec))\n",
    "    min_required = int(np.floor(expected_per_unit * per_unit_min_ratio))\n",
    "\n",
    "    cnt = df.groupby([\"slot\", \"Auto Id\"])[\"Tod\"].size().unstack(\"Auto Id\")\n",
    "    has_all_units = cnt.notna().sum(axis=1) == n_units\n",
    "    enough_each = (cnt >= min_required).all(axis=1)\n",
    "    valid_slots = has_all_units & enough_each\n",
    "\n",
    "    unit_mean = df.groupby([\"slot\", \"Auto Id\"])[\"Tod\"].mean().unstack(\"Auto Id\")\n",
    "\n",
    "    tod_1h = pd.Series(index=unit_mean.index, dtype=float)\n",
    "    ok = valid_slots.reindex(unit_mean.index).fillna(False)\n",
    "    tod_1h.loc[ok] = unit_mean.loc[ok].mean(axis=1)\n",
    "    tod_1h.loc[~ok] = np.nan\n",
    "\n",
    "    return pd.DataFrame({\"Tod_1h\": tod_1h}).reindex(master_index)\n",
    "\n",
    "# ============================================================\n",
    "# 8) WEATHER 전처리 (Temperature/Humidity 1시간 정렬)\n",
    "# ============================================================\n",
    "def preprocess_weather_1h(weather_csv: Path, master_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    dfw = pd.read_csv(weather_csv)\n",
    "    if \"Time\" not in dfw.columns:\n",
    "        raise ValueError(f\"날씨 CSV에 'Time' 컬럼이 없습니다. columns={dfw.columns.tolist()}\")\n",
    "    for c in [\"Temperature\", \"Humidity\"]:\n",
    "        if c not in dfw.columns:\n",
    "            raise ValueError(f\"날씨 CSV에 '{c}' 컬럼이 없습니다. columns={dfw.columns.tolist()}\")\n",
    "\n",
    "    dfw = dfw.copy()\n",
    "    dfw[\"datetime\"] = pd.to_datetime(dfw[\"Time\"], errors=\"coerce\")\n",
    "    dfw = dfw.dropna(subset=[\"datetime\"]).set_index(\"datetime\").sort_index()\n",
    "\n",
    "    w1h = dfw[[\"Temperature\", \"Humidity\"]].resample(\"1h\").mean()\n",
    "    w1h = w1h.reindex(master_index)\n",
    "    return w1h\n",
    "\n",
    "# ============================================================\n",
    "# 9) Fourier feature (자정 롤링)\n",
    "# ============================================================\n",
    "def make_fourier_design(T: int, K: int, s: float) -> np.ndarray:\n",
    "    t = np.arange(T, dtype=float)\n",
    "    X = np.ones((T, 1 + 2*K), dtype=float)\n",
    "    for k in range(1, K+1):\n",
    "        X[:, k]     = np.sin(2*np.pi*k*t/s)\n",
    "        X[:, K + k] = np.cos(2*np.pi*k*t/s)\n",
    "    return X\n",
    "\n",
    "def fit_fourier_ridge(y: np.ndarray, K: int, s: float, l2: float) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "    X = make_fourier_design(len(y), K, s)\n",
    "    D = X.shape[1]\n",
    "    theta = np.linalg.solve(X.T @ X + l2*np.eye(D), X.T @ y)\n",
    "    return theta\n",
    "\n",
    "def build_midnight_fourier_feature(\n",
    "    power_1h: pd.Series,\n",
    "    index: pd.DatetimeIndex,\n",
    "    fit_hours: int,\n",
    "    pred_hours: int,\n",
    "    K: int,\n",
    "    s: float,\n",
    "    l2: float\n",
    ") -> pd.Series:\n",
    "    out = pd.Series(index=index, dtype=float)\n",
    "    power = power_1h.reindex(index)\n",
    "\n",
    "    midnights = [t for t in index if t.hour == 0 and t.minute == 0]\n",
    "    for t0 in midnights:\n",
    "        fit_start = t0 - pd.Timedelta(hours=fit_hours)\n",
    "        fit_end   = t0\n",
    "\n",
    "        y_fit = power.loc[(power.index >= fit_start) & (power.index < fit_end)].values\n",
    "        if len(y_fit) != fit_hours:\n",
    "            continue\n",
    "        if np.any(~np.isfinite(y_fit)):\n",
    "            continue\n",
    "\n",
    "        theta = fit_fourier_ridge(y_fit, K=K, s=s, l2=l2)\n",
    "\n",
    "        T_pred = fit_hours + pred_hours\n",
    "        X_pred = make_fourier_design(T_pred, K, s)\n",
    "        y_hat_24 = (X_pred @ theta)[fit_hours:fit_hours+pred_hours]\n",
    "\n",
    "        pred_idx = pd.date_range(start=t0, periods=pred_hours, freq=\"1h\").intersection(index)\n",
    "        out.loc[pred_idx] = y_hat_24[:len(pred_idx)]\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 10) 전체 파이프라인\n",
    "# ============================================================\n",
    "def main():\n",
    "    # 1) 시간 feature\n",
    "    df_time = create_time_features_1h(master_index)\n",
    "\n",
    "    # 2) SMARTCARE 로드 (전체기간)\n",
    "    df_smart_raw = load_all_csv_time_from_filename(SMARTCARE_DIR, \"LOG_SMARTCARE_*.csv\")\n",
    "    if df_smart_raw is None:\n",
    "        raise RuntimeError(\"SMARTCARE 로그를 읽지 못했습니다. 경로/패턴 확인 필요\")\n",
    "\n",
    "    raw_start = start - pd.Timedelta(\"1h\")  # ceil 때문에 여유\n",
    "    df_smart_raw = df_smart_raw.loc[(df_smart_raw.index >= raw_start) & (df_smart_raw.index <= end)]\n",
    "    smart_1h = preprocess_smartcare_tod_1h(\n",
    "        df_smart_raw,\n",
    "        master_index=master_index,\n",
    "        n_units=N_UNITS,\n",
    "        sample_interval_sec=SMART_SAMPLE_SEC,\n",
    "        freq=\"1h\",\n",
    "        per_unit_min_ratio=SMART_PER_UNIT_MIN_RATIO\n",
    "    )\n",
    "\n",
    "    # 3) EREPORT 로드 (전체기간)\n",
    "    df_ereport_raw = load_all_csv_time_from_filename(EREPORT_DIR, \"DBG_EREPORT_*.csv\")\n",
    "    if df_ereport_raw is None:\n",
    "        raise RuntimeError(\"EREPORT 로그를 읽지 못했습니다. 경로/패턴 확인 필요\")\n",
    "\n",
    "    # ✅ 핵심: forced_off는 preprocess 내부에서 단 1회만 적용\n",
    "    power_1h = preprocess_ereport_power_1h(\n",
    "        df_ereport_raw,\n",
    "        master_index=master_index,\n",
    "        freq=\"1h\",\n",
    "        coverage_threshold=POWER_COVERAGE_THRESHOLD,\n",
    "        forced_zero_ranges=FORCED_OFF_RANGES,\n",
    "    )\n",
    "\n",
    "    # 4) WEATHER 1h\n",
    "    weather_1h = preprocess_weather_1h(WEATHER_CSV, master_index)\n",
    "\n",
    "    # 5) Fourier feature (power 기반)\n",
    "    fourier_midnight = build_midnight_fourier_feature(\n",
    "        power_1h[\"Power_1h\"],\n",
    "        index=master_index,\n",
    "        fit_hours=FOURIER_FIT_HOURS,\n",
    "        pred_hours=24,\n",
    "        K=FOURIER_K,\n",
    "        s=float(FOURIER_S),\n",
    "        l2=FOURIER_RIDGE_L2\n",
    "    )\n",
    "\n",
    "    # 6) merge\n",
    "    df_all = (\n",
    "        df_time\n",
    "        .join(smart_1h, how=\"left\")\n",
    "        .join(power_1h, how=\"left\")\n",
    "        .join(weather_1h, how=\"left\")\n",
    "    )\n",
    "    df_all[\"fourier_base_midnight\"] = fourier_midnight\n",
    "\n",
    "    # 7) 디버그 출력: 문제 구간 확인\n",
    "    print(\"[INFO] Final shape:\", df_all.shape)\n",
    "    print(df_all.loc[\"2025-08-11 18:00:00\":\"2025-08-13 06:00:00\",\n",
    "                     [\"is_holiday\",\"Power_1h\",\"fourier_base_midnight\"]].head(60))\n",
    "\n",
    "    # 8) 저장\n",
    "    df_all.to_csv(OUT_CSV, index_label=\"datetime\")\n",
    "    print(f\"[INFO] Saved -> {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서울대 전처라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "서울대 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import io\n",
    "\n",
    "# ============================================================\n",
    "# 0) 기본 설정 (1시간 기준)\n",
    "# ============================================================\n",
    "SMARTCARE_DIR = Path(\"/home/hd/Desktop/LOG_SMARTCARE_20250930_snu\")\n",
    "EREPORT_DIR   = Path(\"/home/hd/Desktop/LOG_EREPORT_snu\")\n",
    "\n",
    "WEATHER_CSV = Path(\"/home/hd/Desktop/LG/OBS_ASOS_TIM_20250701_20250930_snu.csv\")\n",
    "\n",
    "OUT_CSV = \"./preprocessed_1h_master_with_weather_delta_20250813_20250930_snu.csv\"\n",
    "\n",
    "start = pd.Timestamp(\"2025-07-01 00:00:00\")\n",
    "end   = pd.Timestamp(\"2025-09-30 23:00:00\")\n",
    "master_index = pd.date_range(start=start, end=end, freq=\"1H\")\n",
    "\n",
    "# 사용자 정의 휴일(날짜만)\n",
    "HOLIDAY_DATES = pd.to_datetime([\n",
    "    # 7~8월\n",
    "    \"2025-07-05\",\n",
    "    \"2025-07-06\", \"2025-07-12\",\n",
    "    \"2025-07-13\", \"2025-07-19\",\n",
    "    \"2025-07-20\",\n",
    "    \"2025-07-26\", \"2025-07-27\",\n",
    "    \"2025-08-02\", \"2025-08-03\",\n",
    "    \"2025-08-09\", \"2025-08-10\",\n",
    "\n",
    "    # 8~9월\n",
    "    \"2025-08-15\", \"2025-08-16\", \"2025-08-17\",\n",
    "    \"2025-08-23\", \"2025-08-24\", \"2025-08-30\", \"2025-08-31\",\n",
    "    \"2025-09-06\", \"2025-09-07\", \"2025-09-13\", \"2025-09-14\",\n",
    "    \"2025-09-20\", \"2025-09-21\", \"2025-09-27\", \"2025-09-28\"\n",
    "]).date\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Power coverage / Smartcare per-unit coverage\n",
    "# ------------------------------------------------------------\n",
    "POWER_COVERAGE_THRESHOLD = 0.85   # 60분 중 최소 관측 비율\n",
    "SMART_PER_UNIT_MIN_RATIO = 0.70   # 1시간 중 유닛별 최소 관측 비율\n",
    "N_UNITS = 7\n",
    "SMART_SAMPLE_SEC = 5\n",
    "\n",
    "# ---- Fourier(자정 롤링) 설정\n",
    "FOURIER_S = 168.0              # weekly period in hours (원하면 24.0으로 바꿀 수 있음)\n",
    "FOURIER_K = 10                # 요청값 유지 (다만 파라미터 수가 매우 큼: 1+2K=337)\n",
    "FOURIER_FIT_HOURS = 14 * 24    # 2 weeks = 336\n",
    "FOURIER_RIDGE_L2 = 1e-2        # 안정화용 ridge(특히 K=168이면 권장)\n",
    "\n",
    "# ============================================================\n",
    "# 1) 파일 이름에서 날짜(YYYYMMDD) 뽑기\n",
    "# ============================================================\n",
    "def extract_date_from_filename(filename: str) -> pd.Timestamp:\n",
    "    m = re.search(r\"(\\d{8})\", filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"파일명에서 날짜(YYYYMMDD)를 찾을 수 없습니다: {filename}\")\n",
    "    return pd.to_datetime(m.group(1), format=\"%Y%m%d\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) Time 컬럼과 파일명 날짜로 datetime 만들기\n",
    "# ============================================================\n",
    "def make_datetime_from_time_and_filename(df: pd.DataFrame, file_path: Path) -> pd.DataFrame:\n",
    "    if \"Time\" not in df.columns:\n",
    "        raise ValueError(f\"'Time' 컬럼이 없습니다. 파일: {file_path}\")\n",
    "\n",
    "    file_date = extract_date_from_filename(file_path.name)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        file_date.strftime(\"%Y-%m-%d\") + \" \" + df[\"Time\"].astype(str),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "    df = df.dropna(subset=[\"datetime\"])\n",
    "    df = df.sort_values(\"datetime\").set_index(\"datetime\")\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3) NULL(\\x00) 제거 후 read_csv\n",
    "# ============================================================\n",
    "def read_csv_remove_nulls(path: Path) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as fh:\n",
    "        raw = fh.read()\n",
    "    if b\"\\x00\" in raw:\n",
    "        print(f\"[WARN] NULL 바이트 감지 → 제거 후 로드: {path.name}\")\n",
    "        raw = raw.replace(b\"\\x00\", b\"\")\n",
    "    return pd.read_csv(io.BytesIO(raw))\n",
    "\n",
    "# ============================================================\n",
    "# 4) 폴더 안 CSV를 모두 읽어서 concat\n",
    "# ============================================================\n",
    "def load_all_csv_time_from_filename(folder: Path, pattern: str) -> pd.DataFrame | None:\n",
    "    files = sorted(folder.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"[WARN] {folder} 에 {pattern}에 해당하는 파일이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df_raw = read_csv_remove_nulls(f)\n",
    "            df = make_datetime_from_time_and_filename(df_raw, f)\n",
    "            df[\"src_file\"] = f.name\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] {f} 로드 실패: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        return None\n",
    "\n",
    "    return pd.concat(dfs, axis=0).sort_index()\n",
    "\n",
    "# ============================================================\n",
    "# 5) 시간 feature 생성 (1시간 기준)\n",
    "# ============================================================\n",
    "def create_time_features_1h(index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    df_time = pd.DataFrame(index=index)\n",
    "\n",
    "    df_time[\"date\"] = df_time.index.date\n",
    "    df_time[\"is_holiday\"] = df_time[\"date\"].isin(set(HOLIDAY_DATES)).astype(int)\n",
    "\n",
    "    hour_of_day = df_time.index.hour.astype(int)\n",
    "    df_time[\"day_sin\"] = np.sin(2 * np.pi * hour_of_day / 24.0)\n",
    "    df_time[\"day_cos\"] = np.cos(2 * np.pi * hour_of_day / 24.0)\n",
    "\n",
    "    # week cycle: \"master_index의 시작을 0으로 한\" 위상\n",
    "    k = np.arange(len(df_time), dtype=float)\n",
    "    df_time[\"week_sin\"] = np.sin(2 * np.pi * k / 168.0)\n",
    "    df_time[\"week_cos\"] = np.cos(2 * np.pi * k / 168.0)\n",
    "\n",
    "    return df_time.drop(columns=[\"date\"])\n",
    "\n",
    "# ============================================================\n",
    "# 6) EREPORT 전처리 (1시간 Power 합산 + 결측 보정)\n",
    "# ============================================================\n",
    "def preprocess_ereport_power_1h(df_ereport: pd.DataFrame,\n",
    "                               freq: str = \"1h\",\n",
    "                               coverage_threshold: float = 0.85) -> pd.DataFrame:\n",
    "    POWER_COL = \"Power\"\n",
    "    if POWER_COL not in df_ereport.columns:\n",
    "        raise ValueError(f\"EREPORT에 '{POWER_COL}' 컬럼이 없습니다. 현재 컬럼: {df_ereport.columns.tolist()}\")\n",
    "\n",
    "    df = df_ereport.copy()\n",
    "    df[\"slot\"] = df.index.ceil(freq)\n",
    "\n",
    "    diffs = df.index.to_series().diff().dropna().dt.total_seconds()\n",
    "    if len(diffs) == 0:\n",
    "        raise RuntimeError(\"EREPORT 로그에서 시간 간격을 추정할 수 없습니다.\")\n",
    "    median_step = diffs.median()\n",
    "    if median_step <= 0 or np.isnan(median_step):\n",
    "        median_step = 60.0\n",
    "\n",
    "    expected_rows = int(round(3600 / median_step))\n",
    "    expected_rows = max(expected_rows, 1)\n",
    "    min_required_rows = int(expected_rows * coverage_threshold)\n",
    "\n",
    "    grp_sum = df.groupby(\"slot\")[POWER_COL].sum()\n",
    "    grp_n   = df.groupby(\"slot\")[POWER_COL].size()\n",
    "\n",
    "    out = pd.DataFrame(index=grp_sum.index)\n",
    "    out[\"Power_1h_rawsum\"] = grp_sum\n",
    "    out[\"n_rows\"] = grp_n\n",
    "    out[\"Power_1h\"] = np.nan\n",
    "\n",
    "    for slot, row in out.iterrows():\n",
    "        n = int(row[\"n_rows\"])\n",
    "        if n < min_required_rows:\n",
    "            continue\n",
    "\n",
    "        P_obs = float(row[\"Power_1h_rawsum\"])\n",
    "        m = expected_rows - n\n",
    "        if m <= 0:\n",
    "            out.at[slot, \"Power_1h\"] = P_obs\n",
    "            continue\n",
    "\n",
    "        sub = df[df[\"slot\"] == slot][[POWER_COL]].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        w_end = pd.Timestamp(slot)\n",
    "        w_start = w_end - pd.Timedelta(freq)\n",
    "\n",
    "        step = pd.Timedelta(seconds=float(median_step))\n",
    "        grid = pd.date_range(start=w_start + step, end=w_end, freq=step)\n",
    "\n",
    "        obs_set = set(sub.index)\n",
    "        missing_times = [t for t in grid if t not in obs_set]\n",
    "        m_cnt = m  # 당신 정의대로 N-n 사용\n",
    "\n",
    "        obs_series = sub[POWER_COL].sort_index()\n",
    "        past_nonzero = False\n",
    "        future_nonzero = False\n",
    "\n",
    "        probe = missing_times\n",
    "        if len(probe) > 10:\n",
    "            probe = probe[:: max(1, len(probe)//10)]\n",
    "\n",
    "        for tmiss in probe:\n",
    "            past = obs_series.loc[:tmiss]\n",
    "            if len(past) > 0 and float(past.iloc[-1]) != 0.0:\n",
    "                past_nonzero = True\n",
    "\n",
    "            fut = obs_series.loc[tmiss:]\n",
    "            if len(fut) > 0 and float(fut.iloc[0]) != 0.0:\n",
    "                future_nonzero = True\n",
    "\n",
    "            if past_nonzero and future_nonzero:\n",
    "                break\n",
    "\n",
    "        if past_nonzero and future_nonzero:\n",
    "            P_adj = P_obs * (expected_rows / n)\n",
    "        elif past_nonzero ^ future_nonzero:\n",
    "            avg_obs = P_obs / max(n, 1)\n",
    "            P_adj = P_obs + 0.5 * avg_obs * m_cnt\n",
    "        else:\n",
    "            P_adj = P_obs\n",
    "\n",
    "        out.at[slot, \"Power_1h\"] = P_adj\n",
    "\n",
    "    return out[[\"Power_1h\"]].reindex(master_index)\n",
    "\n",
    "# ============================================================\n",
    "# 7) SMARTCARE 전처리 (1시간 Tod)\n",
    "# ============================================================\n",
    "def preprocess_smartcare_tod_1h(df_smart: pd.DataFrame,\n",
    "                               n_units: int = 11,\n",
    "                               sample_interval_sec: int = 5,\n",
    "                               freq: str = \"1h\",\n",
    "                               per_unit_min_ratio: float = 0.70) -> pd.DataFrame:\n",
    "    REQUIRED_COLS = [\"Auto Id\", \"Tod\"]\n",
    "    missing = [c for c in REQUIRED_COLS if c not in df_smart.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"SMARTCARE에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "    df = df_smart.copy()\n",
    "    df[\"slot\"] = df.index.ceil(freq)\n",
    "\n",
    "    expected_per_unit = int(round(3600 / sample_interval_sec))\n",
    "    min_required = int(np.floor(expected_per_unit * per_unit_min_ratio))\n",
    "\n",
    "    cnt = df.groupby([\"slot\", \"Auto Id\"])[\"Tod\"].size().unstack(\"Auto Id\")\n",
    "    has_all_units = cnt.notna().sum(axis=1) == n_units\n",
    "    enough_each = (cnt >= min_required).all(axis=1)\n",
    "    valid_slots = has_all_units & enough_each\n",
    "\n",
    "    unit_mean = df.groupby([\"slot\", \"Auto Id\"])[\"Tod\"].mean().unstack(\"Auto Id\")\n",
    "\n",
    "    tod_1h = pd.Series(index=unit_mean.index, dtype=float)\n",
    "    ok = valid_slots.reindex(unit_mean.index).fillna(False)\n",
    "    tod_1h.loc[ok] = unit_mean.loc[ok].mean(axis=1)\n",
    "    tod_1h.loc[~ok] = np.nan\n",
    "\n",
    "    return pd.DataFrame({\"Tod_1h\": tod_1h}).reindex(master_index)\n",
    "\n",
    "# ============================================================\n",
    "# 8) WEATHER 전처리 (Temperature/Humidity 원본값을 1시간 row에 붙임)\n",
    "#    - \"그 시간에 맞는 값\"을 들고 오기 위해\n",
    "#      1) Time 컬럼을 datetime으로 파싱\n",
    "#      2) 1시간 그리드(master_index)에 reindex\n",
    "#      3) 같은 시간에 여러 샘플이 있으면 시간별 평균\n",
    "#    - 원본을 그대로 쓰되, 시간 정렬을 위해 resample/mean만 사용\n",
    "# ============================================================\n",
    "def preprocess_weather_1h(weather_csv: Path,\n",
    "                          master_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    dfw = pd.read_csv(weather_csv)\n",
    "\n",
    "    # 어떤 컬럼명이 올지 확정이 아니라서, 우선 아래 3가지만 강제\n",
    "    if \"Time\" not in dfw.columns:\n",
    "        raise ValueError(f\"날씨 CSV에 'Time' 컬럼이 없습니다. columns={dfw.columns.tolist()}\")\n",
    "    for c in [\"Temperature\", \"Humidity\"]:\n",
    "        if c not in dfw.columns:\n",
    "            raise ValueError(f\"날씨 CSV에 '{c}' 컬럼이 없습니다. columns={dfw.columns.tolist()}\")\n",
    "\n",
    "    dfw = dfw.copy()\n",
    "    dfw[\"datetime\"] = pd.to_datetime(dfw[\"Time\"], errors=\"coerce\")\n",
    "    dfw = dfw.dropna(subset=[\"datetime\"]).set_index(\"datetime\").sort_index()\n",
    "\n",
    "    # 1시간 평균으로 정규화(원본값 그대로 쓰되, 같은 시간 다중 레코드가 있을 수 있어서)\n",
    "    w1h = dfw[[\"Temperature\", \"Humidity\"]].resample(\"1h\").mean()\n",
    "\n",
    "    # master_index에 맞춰 정렬\n",
    "    w1h = w1h.reindex(master_index)\n",
    "\n",
    "    return w1h.rename(columns={\"Temperature\": \"Temperature\", \"Humidity\": \"Humidity\"})\n",
    "\n",
    "# ============================================================\n",
    "# 9) 자정마다 2주 파워로 Fourier(K=168) 피팅 후,\n",
    "#    그 자정의 24시간에 대해 baseline 예측값 생성하여 저장\n",
    "# ============================================================\n",
    "def make_fourier_design(T: int, K: int, s: float) -> np.ndarray:\n",
    "    t = np.arange(T, dtype=float)\n",
    "    X = np.ones((T, 1 + 2*K), dtype=float)\n",
    "    for k in range(1, K+1):\n",
    "        X[:, k]     = np.sin(2*np.pi*k*t/s)\n",
    "        X[:, K + k] = np.cos(2*np.pi*k*t/s)\n",
    "    return X\n",
    "\n",
    "def fit_fourier_ridge(y: np.ndarray, K: int, s: float, l2: float) -> np.ndarray:\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)\n",
    "    X = make_fourier_design(len(y), K, s)\n",
    "    D = X.shape[1]\n",
    "    # ridge: (X'X + l2 I)^{-1} X'y\n",
    "    theta = np.linalg.solve(X.T @ X + l2*np.eye(D), X.T @ y)\n",
    "    return theta\n",
    "\n",
    "def build_midnight_fourier_feature(power_1h: pd.Series,\n",
    "                                   index: pd.DatetimeIndex,\n",
    "                                   fit_hours: int = 336,\n",
    "                                   pred_hours: int = 24,\n",
    "                                   K: int = 168,\n",
    "                                   s: float = 168.0,\n",
    "                                   l2: float = 1e-2) -> pd.Series:\n",
    "    \"\"\"\n",
    "    반환:\n",
    "      fourier_midnight_1h: index와 동일 길이\n",
    "      - 매일 00:00에 직전 fit_hours로 피팅\n",
    "      - 그날 24시간(00~23h)의 fourier 값을 채움\n",
    "      - fit window 부족/NaN 포함이면 해당 day는 NaN 유지\n",
    "    \"\"\"\n",
    "    out = pd.Series(index=index, dtype=float)\n",
    "\n",
    "    power = power_1h.reindex(index)\n",
    "\n",
    "    # master_index 내부의 자정들만\n",
    "    midnights = [t for t in index if t.hour == 0 and t.minute == 0]\n",
    "\n",
    "    for t0 in midnights:\n",
    "        fit_start = t0 - pd.Timedelta(hours=fit_hours)\n",
    "        fit_end   = t0  # exclusive\n",
    "\n",
    "        y_fit = power.loc[(power.index >= fit_start) & (power.index < fit_end)].values\n",
    "\n",
    "        if len(y_fit) != fit_hours:\n",
    "            continue\n",
    "        if np.any(~np.isfinite(y_fit)):\n",
    "            continue\n",
    "\n",
    "        theta = fit_fourier_ridge(y_fit, K=K, s=s, l2=l2)\n",
    "\n",
    "        # 예측용 time index: fit 구간 뒤에 이어붙인다고 보고 t=fit_hours..fit_hours+23\n",
    "        T_pred = fit_hours + pred_hours\n",
    "        X_pred = make_fourier_design(T_pred, K, s)\n",
    "        y_hat_all = X_pred @ theta\n",
    "        y_hat_24 = y_hat_all[fit_hours:fit_hours+pred_hours]\n",
    "\n",
    "        pred_idx = pd.date_range(start=t0, periods=pred_hours, freq=\"1h\").intersection(index)\n",
    "        out.loc[pred_idx] = y_hat_24[:len(pred_idx)]\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 10) 전체 파이프라인\n",
    "# ============================================================\n",
    "def main():\n",
    "    # 1) 시간 feature\n",
    "    df_time = create_time_features_1h(master_index)\n",
    "\n",
    "    # 2) SMARTCARE 로드 & 슬라이싱\n",
    "    df_smart_raw = load_all_csv_time_from_filename(SMARTCARE_DIR, \"LOG_SMARTCARE_*.csv\")\n",
    "    if df_smart_raw is None:\n",
    "        raise RuntimeError(\"SMARTCARE 로그를 읽지 못했습니다. 경로/패턴 확인 필요\")\n",
    "\n",
    "    raw_start = start - pd.Timedelta(\"1h\")  # slot=ceil이므로 직전 1시간 여유\n",
    "    df_smart_raw = df_smart_raw.loc[(df_smart_raw.index >= raw_start) & (df_smart_raw.index <= end)]\n",
    "    smart_1h = preprocess_smartcare_tod_1h(\n",
    "        df_smart_raw,\n",
    "        n_units=N_UNITS,\n",
    "        sample_interval_sec=SMART_SAMPLE_SEC,\n",
    "        freq=\"1h\",\n",
    "        per_unit_min_ratio=SMART_PER_UNIT_MIN_RATIO\n",
    "    )\n",
    "\n",
    "    # 3) EREPORT 로드 & 슬라이싱\n",
    "    df_ereport_raw = load_all_csv_time_from_filename(EREPORT_DIR, \"DBG_EREPORT_*.csv\")\n",
    "    if df_ereport_raw is None:\n",
    "        raise RuntimeError(\"EREPORT 로그를 읽지 못했습니다. 경로/패턴 확인 필요\")\n",
    "\n",
    "    df_ereport_raw = df_ereport_raw.loc[(df_ereport_raw.index >= raw_start) & (df_ereport_raw.index <= end)]\n",
    "    power_1h = preprocess_ereport_power_1h(\n",
    "        df_ereport_raw,\n",
    "        freq=\"1h\",\n",
    "        coverage_threshold=POWER_COVERAGE_THRESHOLD\n",
    "    )\n",
    "\n",
    "    # 4) WEATHER 1h (원본 Temperature/Humidity)\n",
    "    weather_1h = preprocess_weather_1h(WEATHER_CSV, master_index)\n",
    "\n",
    "    # 5) 자정 롤링 Fourier feature (power 기반)\n",
    "    fourier_midnight = build_midnight_fourier_feature(\n",
    "        power_1h[\"Power_1h\"],\n",
    "        index=master_index,\n",
    "        fit_hours=FOURIER_FIT_HOURS,\n",
    "        pred_hours=24,\n",
    "        K=FOURIER_K,\n",
    "        s=float(FOURIER_S),\n",
    "        l2=FOURIER_RIDGE_L2\n",
    "    )\n",
    "\n",
    "    # 6) merge\n",
    "    df_all = (\n",
    "        df_time\n",
    "        .join(smart_1h, how=\"left\")\n",
    "        .join(power_1h, how=\"left\")\n",
    "        .join(weather_1h, how=\"left\")  # Temperature, Humidity\n",
    "    )\n",
    "    df_all[\"fourier_base_midnight\"] = fourier_midnight\n",
    "\n",
    "    print(\"[INFO] Final shape:\", df_all.shape)\n",
    "    print(df_all.head(3))\n",
    "    print(df_all.tail(3))\n",
    "\n",
    "    df_all.to_csv(OUT_CSV, index_label=\"datetime\")\n",
    "    print(f\"[INFO] Saved -> {OUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n주 학습 1주 발리드 윈도우 (전처리 파일 경로 바꿔야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Rolling (weekly step) training (UPDATED: \"latest run only\" support)\n",
    "# - buffer + train + valid windows, weekly shift\n",
    "# - Train model per window and save checkpoints:\n",
    "#     (1) EXP_DIR/run_xxxx.../best.pt, last.pt\n",
    "#     (2) EXP_DIR/ckpts_flat/best_wXXXX_...pt, last_wXXXX_...pt  (sortable)\n",
    "# - Write RUNS_DIR/LATEST_RUN.json so eval code can load ONLY the newest run\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CSV_PATH = \"./preprocessed_1h_master_with_weather_delta_20250701_20250930_OS.csv\"\n",
    "DT_COL = \"datetime\"\n",
    "\n",
    "TARGET_COL = \"Power_1h\"\n",
    "\n",
    "EXOG_COLS_BASE = [\"is_holiday\", \"day_sin\", \"day_cos\"]\n",
    "FOURIER_COL = \"fourier_base_midnight\"\n",
    "WEATHER_COLS = [\"Temperature\", \"Humidity\"]\n",
    "\n",
    "USE_TOD = False\n",
    "TOD_COL = \"Tod_1h\"\n",
    "\n",
    "LOOKBACK = 168\n",
    "HORIZON  = 24\n",
    "\n",
    "# rolling window config\n",
    "BUFFER_HOURS = 14 * 24        # 2 weeks buffer\n",
    "TRAIN_HOURS  = 28 * 24        # 4 weeks (as in your code)\n",
    "VALID_HOURS  =  7 * 24        # 1 week\n",
    "STEP_HOURS   =  7 * 24        # shift by 1 week\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 400\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "GRAD_CLIP = 5.0\n",
    "SEED = 42\n",
    "\n",
    "# ---- Loss weights\n",
    "LAMBDA_DIFF  = 1.0\n",
    "LAMBDA_COS   = 1.0\n",
    "LAMBDA_SHARE = 0.0\n",
    "\n",
    "COS_EPS = 1e-8\n",
    "SHARE_EPS = 1e-6\n",
    "DAY_SUM_MASK_EPS = 1e-3\n",
    "TOP_ALPHA = 0.20\n",
    "\n",
    "RUNS_DIR = \"./runs_lstm24_roll\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_all(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_all(SEED)\n",
    "\n",
    "# ============================================================\n",
    "# ✅ NEW: \"this execution only\" experiment folder + latest pointer\n",
    "# ============================================================\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "EXP_DIR = os.path.join(RUNS_DIR, f\"exp_{RUN_ID}\")\n",
    "os.makedirs(EXP_DIR, exist_ok=True)\n",
    "\n",
    "CKPT_FLAT_DIR = os.path.join(EXP_DIR, \"ckpts_flat\")\n",
    "os.makedirs(CKPT_FLAT_DIR, exist_ok=True)\n",
    "\n",
    "LATEST_JSON = os.path.join(RUNS_DIR, \"LATEST_RUN.json\")\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"exp_dir\": os.path.abspath(EXP_DIR),\n",
    "    \"ckpts\": [],  # append {\"type\":\"last\"/\"best\", \"wi\":int, \"path\":abs_path}\n",
    "}\n",
    "\n",
    "print(f\"[INFO] RUN_ID={RUN_ID}\")\n",
    "print(f\"[INFO] EXP_DIR={EXP_DIR}\")\n",
    "print(f\"[INFO] CKPT_FLAT_DIR={CKPT_FLAT_DIR}\")\n",
    "print(f\"[INFO] LATEST_JSON will be written to: {LATEST_JSON}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) Load CSV (NO global dropna)\n",
    "# ============================================================\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[DT_COL]).set_index(DT_COL).sort_index()\n",
    "\n",
    "req_min = [TARGET_COL] + EXOG_COLS_BASE + [FOURIER_COL] + WEATHER_COLS\n",
    "if USE_TOD:\n",
    "    req_min += [TOD_COL]\n",
    "missing = [c for c in req_min if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV에 필요한 컬럼이 없습니다: {missing}\")\n",
    "\n",
    "EXOG_COLS = [FOURIER_COL] + WEATHER_COLS + EXOG_COLS_BASE\n",
    "if USE_TOD:\n",
    "    EXOG_COLS = [FOURIER_COL] + WEATHER_COLS + [TOD_COL] + EXOG_COLS_BASE\n",
    "\n",
    "df_feat = df[[TARGET_COL] + EXOG_COLS].copy()\n",
    "\n",
    "y = df_feat[TARGET_COL].values.astype(np.float32)\n",
    "xe = df_feat[EXOG_COLS].values.astype(np.float32)\n",
    "idx = df_feat.index\n",
    "N = len(df_feat)\n",
    "\n",
    "is_finite_y  = np.isfinite(y)\n",
    "is_finite_xe = np.isfinite(xe).all(axis=1)\n",
    "\n",
    "# ============================================================\n",
    "# 2) Dataset\n",
    "# ============================================================\n",
    "class Power24Dataset(Dataset):\n",
    "    def __init__(self, y: np.ndarray, xe: np.ndarray, t_list: np.ndarray, lookback: int, horizon: int):\n",
    "        self.y = y\n",
    "        self.xe = xe\n",
    "        self.t_list = t_list\n",
    "        self.lookback = int(lookback)\n",
    "        self.horizon = int(horizon)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.t_list)\n",
    "\n",
    "    def __getitem__(self, idx_):\n",
    "        t = int(self.t_list[idx_])\n",
    "        x_power = self.y[t-self.lookback:t][:, None]      # (L,1)\n",
    "        x_exog  = self.xe[t:t+self.horizon, :]            # (H,d)\n",
    "        y_out   = self.y[t:t+self.horizon]                # (H,)\n",
    "        return torch.from_numpy(x_power), torch.from_numpy(x_exog), torch.from_numpy(y_out)\n",
    "\n",
    "# ============================================================\n",
    "# 3) Model\n",
    "# ============================================================\n",
    "class LSTM24(nn.Module):\n",
    "    def __init__(self, exog_dim: int, horizon: int, hidden: int = 32):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.hidden = hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.exog_mlp = nn.Sequential(\n",
    "            nn.Linear(horizon * exog_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2 * hidden + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, horizon),\n",
    "        )\n",
    "\n",
    "        self.out_act = nn.Softplus(beta=1.0, threshold=20.0)\n",
    "\n",
    "    def forward(self, x_power, x_exog_future):\n",
    "        _, (h_n, c_n) = self.lstm(x_power)\n",
    "        h = h_n[-1]\n",
    "        c = c_n[-1]\n",
    "        hc = torch.cat([h, c], dim=1)\n",
    "\n",
    "        B = x_exog_future.size(0)\n",
    "        ex = x_exog_future.reshape(B, -1)\n",
    "        ex = self.exog_mlp(ex)\n",
    "\n",
    "        z = torch.cat([hc, ex], dim=1)\n",
    "        y_raw = self.head(z)\n",
    "        return self.out_act(y_raw)\n",
    "\n",
    "# ============================================================\n",
    "# 4) Loss + Metrics\n",
    "# ============================================================\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def diff_mse(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    dy_pred = y_pred[:, 1:] - y_pred[:, :-1]\n",
    "    dy_true = y_true[:, 1:] - y_true[:, :-1]\n",
    "    return (dy_pred - dy_true).pow(2).mean()\n",
    "\n",
    "def cos_centered(y_pred: torch.Tensor, y_true: torch.Tensor, eps: float = COS_EPS) -> torch.Tensor:\n",
    "    yp = y_pred - y_pred.mean(dim=1, keepdim=True)\n",
    "    yt = y_true - y_true.mean(dim=1, keepdim=True)\n",
    "    num = (yp * yt).sum(dim=1)\n",
    "    den = yp.norm(p=2, dim=1) * yt.norm(p=2, dim=1) + eps\n",
    "    return (num / den).mean()\n",
    "\n",
    "def share_tv_loss(y_pred: torch.Tensor, y_true: torch.Tensor,\n",
    "                  eps: float = SHARE_EPS,\n",
    "                  day_sum_mask_eps: float = DAY_SUM_MASK_EPS) -> torch.Tensor:\n",
    "    y_true_pos = torch.clamp(y_true, min=0.0)\n",
    "    y_pred_pos = torch.clamp(y_pred, min=0.0)\n",
    "\n",
    "    sum_t = (y_true_pos + eps).sum(dim=1, keepdim=True)\n",
    "    sum_p = (y_pred_pos + eps).sum(dim=1, keepdim=True)\n",
    "\n",
    "    p = (y_true_pos + eps) / sum_t\n",
    "    q = (y_pred_pos + eps) / sum_p\n",
    "\n",
    "    tv = 0.5 * torch.sum(torch.abs(p - q), dim=1)\n",
    "\n",
    "    w = (sum_t.squeeze(1) >= day_sum_mask_eps).float()\n",
    "    denom = w.sum().clamp(min=1.0)\n",
    "    return (tv * w).sum() / denom\n",
    "\n",
    "def total_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "    base = mse_loss(y_pred, y_true)\n",
    "    dloss = diff_mse(y_pred, y_true)\n",
    "    csim = cos_centered(y_pred, y_true)\n",
    "    clos = 1.0 - csim\n",
    "    sh   = share_tv_loss(y_pred, y_true)\n",
    "    return base + LAMBDA_DIFF * dloss + LAMBDA_COS * clos + LAMBDA_SHARE * sh\n",
    "\n",
    "@torch.no_grad()\n",
    "def topk_iou(y_pred: torch.Tensor, y_true: torch.Tensor, alpha: float = TOP_ALPHA, eps: float = 1e-12) -> torch.Tensor:\n",
    "    B, H = y_true.shape\n",
    "    K = int(np.ceil(alpha * H))\n",
    "    _, idx_t = torch.topk(y_true, k=K, dim=1, largest=True, sorted=False)\n",
    "    _, idx_p = torch.topk(y_pred, k=K, dim=1, largest=True, sorted=False)\n",
    "\n",
    "    mask_t = torch.zeros((B, H), device=y_true.device, dtype=torch.bool)\n",
    "    mask_p = torch.zeros((B, H), device=y_true.device, dtype=torch.bool)\n",
    "    mask_t.scatter_(1, idx_t, True)\n",
    "    mask_p.scatter_(1, idx_p, True)\n",
    "\n",
    "    inter = (mask_t & mask_p).sum(dim=1).float()\n",
    "    union = (mask_t | mask_p).sum(dim=1).float()\n",
    "    return (inter / (union + eps)).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def share_overlap_percent(y_pred: torch.Tensor, y_true: torch.Tensor, eps: float = SHARE_EPS) -> torch.Tensor:\n",
    "    y_true_pos = torch.clamp(y_true, min=0.0)\n",
    "    y_pred_pos = torch.clamp(y_pred, min=0.0)\n",
    "    p = (y_true_pos + eps) / (y_true_pos + eps).sum(dim=1, keepdim=True)\n",
    "    q = (y_pred_pos + eps) / (y_pred_pos + eps).sum(dim=1, keepdim=True)\n",
    "    overlap = torch.minimum(p, q).sum(dim=1)\n",
    "    return 100.0 * overlap.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_all_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    se_sum, n_elem = 0.0, 0\n",
    "    dse_sum, d_n   = 0.0, 0\n",
    "    cos_sum, b_cnt = 0.0, 0\n",
    "    iou_sum        = 0.0\n",
    "    sh_ov_sum      = 0.0\n",
    "    sh_tv_sum      = 0.0\n",
    "\n",
    "    for x_power, x_exog, y_true in dataloader:\n",
    "        x_power = x_power.to(device)\n",
    "        x_exog  = x_exog.to(device)\n",
    "        y_true  = y_true.to(device)\n",
    "\n",
    "        y_pred = model(x_power, x_exog)\n",
    "\n",
    "        diff = y_pred - y_true\n",
    "        se_sum += float((diff**2).sum().item())\n",
    "        n_elem += int(y_true.numel())\n",
    "\n",
    "        dy_pred = y_pred[:, 1:] - y_pred[:, :-1]\n",
    "        dy_true = y_true[:, 1:] - y_true[:, :-1]\n",
    "        dd = dy_pred - dy_true\n",
    "        dse_sum += float((dd**2).sum().item())\n",
    "        d_n     += int(dd.numel())\n",
    "\n",
    "        cos_sum += float(cos_centered(y_pred, y_true).item())\n",
    "        b_cnt   += 1\n",
    "\n",
    "        iou_sum += float(topk_iou(y_pred, y_true).item())\n",
    "        sh_ov_sum += float(share_overlap_percent(y_pred, y_true).item())\n",
    "        sh_tv_sum += float(share_tv_loss(y_pred, y_true).item())\n",
    "\n",
    "    rmse      = float(np.sqrt(se_sum / max(n_elem, 1)))\n",
    "    diff_rmse = float(np.sqrt(dse_sum / max(d_n, 1)))\n",
    "    cos_mean  = float(cos_sum / max(b_cnt, 1))\n",
    "    iou_mean  = float(iou_sum / max(b_cnt, 1))\n",
    "    sh_ov     = float(sh_ov_sum / max(b_cnt, 1))\n",
    "    sh_tv     = float(sh_tv_sum / max(b_cnt, 1))\n",
    "    return rmse, diff_rmse, cos_mean, iou_mean, sh_ov, sh_tv\n",
    "\n",
    "# ============================================================\n",
    "# 5) t_list builder\n",
    "# ============================================================\n",
    "def make_t_list_in_range(t_start_inclusive: int, t_end_exclusive: int) -> np.ndarray:\n",
    "    t_list = []\n",
    "    lo = max(t_start_inclusive, LOOKBACK)\n",
    "    hi = min(t_end_exclusive, N - HORIZON)\n",
    "    for t in range(lo, hi):\n",
    "        if not is_finite_y[t-LOOKBACK:t].all():\n",
    "            continue\n",
    "        if not is_finite_y[t:t+HORIZON].all():\n",
    "            continue\n",
    "        if not is_finite_xe[t:t+HORIZON].all():\n",
    "            continue\n",
    "        t_list.append(t)\n",
    "    return np.array(t_list, dtype=int)\n",
    "\n",
    "def fmt_dt(ts: pd.Timestamp) -> str:\n",
    "    return ts.strftime(\"%Y%m%d\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Build rolling windows\n",
    "# ============================================================\n",
    "train_start0 = BUFFER_HOURS\n",
    "train_end0   = train_start0 + TRAIN_HOURS\n",
    "val_start0   = train_end0\n",
    "val_end0     = val_start0 + VALID_HOURS\n",
    "\n",
    "windows = []\n",
    "while val_end0 <= N:\n",
    "    windows.append((train_start0, train_end0, val_start0, val_end0))\n",
    "    train_start0 += STEP_HOURS\n",
    "    train_end0   += STEP_HOURS\n",
    "    val_start0   += STEP_HOURS\n",
    "    val_end0     += STEP_HOURS\n",
    "\n",
    "if len(windows) == 0:\n",
    "    raise RuntimeError(\"가능한 rolling window가 0개입니다. (데이터 길이/버퍼/윈도우 설정 확인)\")\n",
    "\n",
    "print(f\"[INFO] total rows={N}, total windows={len(windows)}\")\n",
    "print(f\"[INFO] first window train={idx[windows[0][0]]}~{idx[windows[0][1]-1]} | \"\n",
    "      f\"val={idx[windows[0][2]]}~{idx[windows[0][3]-1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# 7) Train per window + save best/last (EXP_DIR only)\n",
    "# ============================================================\n",
    "for wi, (tr_s, tr_e, va_s, va_e) in enumerate(windows, start=1):\n",
    "    t_list_train = make_t_list_in_range(tr_s, tr_e)\n",
    "    t_list_val   = make_t_list_in_range(va_s, va_e)\n",
    "\n",
    "    if len(t_list_train) == 0 or len(t_list_val) == 0:\n",
    "        print(f\"[WARN] window {wi:04d}: skip (train_t={len(t_list_train)}, val_t={len(t_list_val)})\")\n",
    "        continue\n",
    "\n",
    "    tr_start_dt = idx[tr_s]\n",
    "    tr_end_dt   = idx[tr_e-1]\n",
    "    va_start_dt = idx[va_s]\n",
    "    va_end_dt   = idx[va_e-1]\n",
    "\n",
    "    run_name = (\n",
    "        f\"run_{wi:04d}_\"\n",
    "        f\"TR{fmt_dt(tr_start_dt)}-{fmt_dt(tr_end_dt)}_\"\n",
    "        f\"VA{fmt_dt(va_start_dt)}-{fmt_dt(va_end_dt)}\"\n",
    "    )\n",
    "\n",
    "    # ✅ save under EXP_DIR (not RUNS_DIR root)\n",
    "    run_dir = os.path.join(EXP_DIR, run_name)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n========== [{wi:04d}/{len(windows):04d}] {run_name} ==========\")\n",
    "    print(f\"  train_t={len(t_list_train)} | val_t={len(t_list_val)}\")\n",
    "    print(f\"  train_range: {tr_start_dt} ~ {tr_end_dt}\")\n",
    "    print(f\"  valid_range: {va_start_dt} ~ {va_end_dt}\")\n",
    "\n",
    "    ds_train = Power24Dataset(y, xe, t_list_train, LOOKBACK, HORIZON)\n",
    "    ds_val   = Power24Dataset(y, xe, t_list_val,   LOOKBACK, HORIZON)\n",
    "    dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "    dl_val   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    seed_all(SEED)\n",
    "    model = LSTM24(exog_dim=len(EXOG_COLS), horizon=HORIZON, hidden=32).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    best_state = None\n",
    "    best_epoch = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        total_loss_sum, n_batches = 0.0, 0\n",
    "\n",
    "        for x_power, x_exog, y_true in dl_train:\n",
    "            x_power = x_power.to(DEVICE)\n",
    "            x_exog  = x_exog.to(DEVICE)\n",
    "            y_true  = y_true.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            y_pred = model(x_power, x_exog)\n",
    "            loss = total_loss(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss_sum += float(loss.item())\n",
    "            n_batches += 1\n",
    "\n",
    "        train_rmse, train_drmse, train_cos, train_iou, train_shov, train_shtv = eval_all_metrics(model, dl_train, DEVICE)\n",
    "        val_rmse,   val_drmse,   val_cos,   val_iou,   val_shov,   val_shtv   = eval_all_metrics(model, dl_val,   DEVICE)\n",
    "\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            best_epoch = epoch\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        avg_train_loss = total_loss_sum / max(n_batches, 1)\n",
    "\n",
    "        if (epoch == 1) or (epoch % 10 == 0) or (epoch == EPOCHS):\n",
    "            print(\n",
    "                f\"[Epoch {epoch:03d}] \"\n",
    "                f\"train_loss={avg_train_loss:.6f} | \"\n",
    "                f\"train_RMSE={train_rmse:.6f}, dRMSE={train_drmse:.6f}, cosC={train_cos:.4f}, \"\n",
    "                f\"IoU={train_iou:.4f}, shareOv={train_shov:.2f}%, shareTV={train_shtv:.4f} | \"\n",
    "                f\"val_RMSE={val_rmse:.6f}, dRMSE={val_drmse:.6f}, cosC={val_cos:.4f}, \"\n",
    "                f\"IoU={val_iou:.4f}, shareOv={val_shov:.2f}%, shareTV={val_shtv:.4f} | \"\n",
    "                f\"best_val_RMSE={best_val_rmse:.6f} (epoch={best_epoch})\"\n",
    "            )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save (run_dir + ckpts_flat under EXP_DIR)\n",
    "    # -----------------------------\n",
    "    tr_s_ymd = fmt_dt(tr_start_dt)\n",
    "    tr_e_ymd = fmt_dt(tr_end_dt)\n",
    "    va_s_ymd = fmt_dt(va_start_dt)\n",
    "    va_e_ymd = fmt_dt(va_end_dt)\n",
    "\n",
    "    flat_last_name = f\"last_w{wi:04d}_TR{tr_s_ymd}-{tr_e_ymd}_VA{va_s_ymd}-{va_e_ymd}.pt\"\n",
    "    flat_best_name = f\"best_w{wi:04d}_TR{tr_s_ymd}-{tr_e_ymd}_VA{va_s_ymd}-{va_e_ymd}.pt\"\n",
    "\n",
    "    last_path = os.path.join(run_dir, \"last.pt\")\n",
    "    best_path = os.path.join(run_dir, \"best.pt\")\n",
    "\n",
    "    flat_last_path = os.path.join(CKPT_FLAT_DIR, flat_last_name)\n",
    "    flat_best_path = os.path.join(CKPT_FLAT_DIR, flat_best_name)\n",
    "\n",
    "    window_meta = {\n",
    "        \"wi\": wi,\n",
    "        \"train_start\": str(tr_start_dt),\n",
    "        \"train_end\":   str(tr_end_dt),\n",
    "        \"val_start\":   str(va_start_dt),\n",
    "        \"val_end\":     str(va_end_dt),\n",
    "        \"train_hours\": TRAIN_HOURS,\n",
    "        \"val_hours\":   VALID_HOURS,\n",
    "        \"step_hours\":  STEP_HOURS,\n",
    "        \"buffer_hours\": BUFFER_HOURS,\n",
    "    }\n",
    "    config_meta = {\n",
    "        \"LOOKBACK\": LOOKBACK,\n",
    "        \"HORIZON\": HORIZON,\n",
    "        \"TARGET_COL\": TARGET_COL,\n",
    "        \"EXOG_COLS\": EXOG_COLS,\n",
    "        \"USE_TOD\": USE_TOD,\n",
    "        \"LOSS\": f\"MSE + {LAMBDA_DIFF}*diffMSE + {LAMBDA_COS}*(1-cosCentered) + {LAMBDA_SHARE}*shareTV\",\n",
    "        \"TOP_ALPHA\": TOP_ALPHA,\n",
    "        \"SHARE_EPS\": SHARE_EPS,\n",
    "        \"DAY_SUM_MASK_EPS\": DAY_SUM_MASK_EPS,\n",
    "        \"N_TRAIN_T\": int(len(t_list_train)),\n",
    "        \"N_VAL_T\": int(len(t_list_val)),\n",
    "        \"SEED\": SEED,\n",
    "        \"LR\": LR,\n",
    "        \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"GRAD_CLIP\": GRAD_CLIP,\n",
    "        \"MODEL\": \"LSTM24(hidden=32)+exogMLP+Softplus\",\n",
    "    }\n",
    "\n",
    "    last_payload = {\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"opt_state\": opt.state_dict(),\n",
    "        \"best_val_rmse\": best_val_rmse,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"run_name\": run_name,\n",
    "        \"window\": window_meta,\n",
    "        \"config\": config_meta,\n",
    "    }\n",
    "    torch.save(last_payload, last_path)\n",
    "    torch.save(last_payload, flat_last_path)\n",
    "\n",
    "    manifest[\"ckpts\"].append({\"type\": \"last\", \"wi\": int(wi), \"path\": os.path.abspath(flat_last_path)})\n",
    "\n",
    "    if best_state is not None:\n",
    "        best_payload = {\n",
    "            \"epoch\": best_epoch,\n",
    "            \"model_state\": best_state,\n",
    "            \"best_val_rmse\": best_val_rmse,\n",
    "            \"run_name\": run_name,\n",
    "            \"window\": window_meta,\n",
    "            \"config\": config_meta,\n",
    "        }\n",
    "        torch.save(best_payload, best_path)\n",
    "        torch.save(best_payload, flat_best_path)\n",
    "        manifest[\"ckpts\"].append({\"type\": \"best\", \"wi\": int(wi), \"path\": os.path.abspath(flat_best_path)})\n",
    "        print(f\"[SAVE] best -> {best_path}\")\n",
    "        print(f\"[SAVE] best(flat) -> {flat_best_path}\")\n",
    "    else:\n",
    "        print(f\"[WARN] {run_name}: best_state is None -> best.pt not saved\")\n",
    "\n",
    "    print(f\"[SAVE] last -> {last_path}\")\n",
    "    print(f\"[SAVE] last(flat) -> {flat_last_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# ✅ NEW: Write latest pointer JSON (so eval loads ONLY newest run)\n",
    "# ============================================================\n",
    "with open(LATEST_JSON, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"\\n[INFO] wrote latest run pointer -> {LATEST_JSON}\")\n",
    "print(f\"[INFO] this run ckpts count = {len(manifest['ckpts'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Jupyter-friendly evaluation (UPDATED: ONLY latest run)\n",
    "# - Reads RUNS_DIR/LATEST_RUN.json written by the UPDATED training code\n",
    "# - Evaluates ONLY the \"last\" ckpts created in that latest run\n",
    "# - On each model: evaluate on its OWN val range\n",
    "# - midnight-only samples (00:00 start -> next 24h)\n",
    "# - SHOW plots inline (VS Code Jupyter)\n",
    "# ============================================================\n",
    "\n",
    "import os, json, math, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CSV_PATH = \"./preprocessed_1h_master_with_weather_delta_20250701_20250930_OS.csv\"\n",
    "DT_COL = \"datetime\"\n",
    "TARGET_COL = \"Power_1h\"\n",
    "\n",
    "RUNS_DIR = \"./runs_lstm24_roll\"\n",
    "LATEST_JSON = os.path.join(RUNS_DIR, \"LATEST_RUN.json\")  # ✅ latest pointer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "\n",
    "SHOW_PLOTS = True\n",
    "SAVE_PLOTS = False\n",
    "MAX_DAYS_PER_MODEL = 7\n",
    "\n",
    "OUT_DIR = os.path.join(RUNS_DIR, \"eval_last_midnight_on_own_val_inline_latest\")\n",
    "PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# Load dataset once\n",
    "# ============================================================\n",
    "df = pd.read_csv(CSV_PATH, parse_dates=[DT_COL]).set_index(DT_COL).sort_index()\n",
    "\n",
    "# ============================================================\n",
    "# Model (must match training)\n",
    "# ============================================================\n",
    "class LSTM24(nn.Module):\n",
    "    def __init__(self, exog_dim: int, horizon: int, hidden: int = 32):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=hidden, num_layers=1, batch_first=True)\n",
    "        self.exog_mlp = nn.Sequential(\n",
    "            nn.Linear(horizon * exog_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2 * hidden + 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, horizon),\n",
    "        )\n",
    "        self.out_act = nn.Softplus(beta=1.0, threshold=20.0)\n",
    "\n",
    "    def forward(self, x_power, x_exog_future):\n",
    "        _, (h_n, c_n) = self.lstm(x_power)\n",
    "        h = h_n[-1]\n",
    "        c = c_n[-1]\n",
    "        hc = torch.cat([h, c], dim=1)\n",
    "\n",
    "        B = x_exog_future.size(0)\n",
    "        ex = x_exog_future.reshape(B, -1)\n",
    "        ex = self.exog_mlp(ex)\n",
    "\n",
    "        z = torch.cat([hc, ex], dim=1)\n",
    "        return self.out_act(self.head(z))\n",
    "\n",
    "# ============================================================\n",
    "# Helpers (numpy metrics)\n",
    "# ============================================================\n",
    "TOP_ALPHA = 0.20\n",
    "COS_EPS = 1e-8\n",
    "SHARE_EPS = 1e-6\n",
    "DAY_SUM_MASK_EPS = 1e-3\n",
    "\n",
    "def diff_rmse_np(y_pred, y_true):\n",
    "    dy_p = y_pred[:, 1:] - y_pred[:, :-1]\n",
    "    dy_t = y_true[:, 1:] - y_true[:, :-1]\n",
    "    return float(np.sqrt(np.mean((dy_p - dy_t) ** 2)))\n",
    "\n",
    "def cos_centered_np(y_pred, y_true, eps=COS_EPS):\n",
    "    yp = y_pred - y_pred.mean(axis=1, keepdims=True)\n",
    "    yt = y_true - y_true.mean(axis=1, keepdims=True)\n",
    "    num = np.sum(yp * yt, axis=1)\n",
    "    den = (np.linalg.norm(yp, axis=1) * np.linalg.norm(yt, axis=1) + eps)\n",
    "    return float(np.mean(num / den))\n",
    "\n",
    "def topk_iou_np(y_pred, y_true, alpha=TOP_ALPHA, eps=1e-12):\n",
    "    M, H = y_true.shape\n",
    "    K = int(math.ceil(alpha * H))\n",
    "    ious = []\n",
    "    for i in range(M):\n",
    "        idx_t = np.argpartition(-y_true[i], K-1)[:K]\n",
    "        idx_p = np.argpartition(-y_pred[i], K-1)[:K]\n",
    "        set_t, set_p = set(idx_t.tolist()), set(idx_p.tolist())\n",
    "        inter = len(set_t & set_p)\n",
    "        union = len(set_t | set_p)\n",
    "        ious.append(inter / (union + eps))\n",
    "    return float(np.mean(ious))\n",
    "\n",
    "def share_overlap_percent_np(y_pred, y_true, eps=SHARE_EPS):\n",
    "    y_true_pos = np.clip(y_true, 0.0, None)\n",
    "    y_pred_pos = np.clip(y_pred, 0.0, None)\n",
    "    p = (y_true_pos + eps) / np.sum(y_true_pos + eps, axis=1, keepdims=True)\n",
    "    q = (y_pred_pos + eps) / np.sum(y_pred_pos + eps, axis=1, keepdims=True)\n",
    "    overlap = np.sum(np.minimum(p, q), axis=1)\n",
    "    return float(100.0 * np.mean(overlap))\n",
    "\n",
    "def share_tv_np(y_pred, y_true, eps=SHARE_EPS, day_sum_mask_eps=DAY_SUM_MASK_EPS):\n",
    "    y_true_pos = np.clip(y_true, 0.0, None)\n",
    "    y_pred_pos = np.clip(y_pred, 0.0, None)\n",
    "    sum_t = np.sum(y_true_pos + eps, axis=1, keepdims=True)\n",
    "    p = (y_true_pos + eps) / sum_t\n",
    "    q = (y_pred_pos + eps) / np.sum(y_pred_pos + eps, axis=1, keepdims=True)\n",
    "    tv = 0.5 * np.sum(np.abs(p - q), axis=1)\n",
    "    w = (sum_t.squeeze(1) >= day_sum_mask_eps).astype(np.float32)\n",
    "    denom = max(float(np.sum(w)), 1.0)\n",
    "    return float(np.sum(tv * w) / denom)\n",
    "\n",
    "def _strict_index_loc(idx_dt: pd.DatetimeIndex, ts: pd.Timestamp) -> int:\n",
    "    \"\"\"\n",
    "    Make sure val_start/val_end are exactly on idx_dt.\n",
    "    If not present (e.g. timezone mismatch), fallback to nearest but warn.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loc = idx_dt.get_loc(ts)\n",
    "        if isinstance(loc, slice):\n",
    "            return int(loc.start)\n",
    "        if isinstance(loc, (np.ndarray, list)):\n",
    "            return int(loc[0])\n",
    "        return int(loc)\n",
    "    except KeyError:\n",
    "        # fallback\n",
    "        loc = int(idx_dt.get_indexer([ts], method=\"nearest\")[0])\n",
    "        print(f\"[WARN] timestamp not exactly in index: {ts} -> using nearest {idx_dt[loc]}\")\n",
    "        return loc\n",
    "\n",
    "def build_midnight_val_t_list(idx_dt, y, xe, lookback, horizon, val_start_ts, val_end_ts):\n",
    "    \"\"\"\n",
    "    Build t_list within [val_start_ts, val_end_ts] inclusive,\n",
    "    only for t at midnight (00:00), and with full finite past/future.\n",
    "    \"\"\"\n",
    "    va_s = _strict_index_loc(idx_dt, pd.Timestamp(val_start_ts))\n",
    "    va_e_incl = _strict_index_loc(idx_dt, pd.Timestamp(val_end_ts))\n",
    "    va_e = va_e_incl + 1  # exclusive\n",
    "\n",
    "    N = len(idx_dt)\n",
    "    is_finite_y  = np.isfinite(y)\n",
    "    is_finite_xe = np.isfinite(xe).all(axis=1)\n",
    "\n",
    "    t_list = []\n",
    "    lo = max(va_s, lookback)\n",
    "    hi = min(va_e, N - horizon)\n",
    "    for t in range(lo, hi):\n",
    "        if idx_dt[t].hour != 0:\n",
    "            continue\n",
    "        if not is_finite_y[t-lookback:t].all():\n",
    "            continue\n",
    "        if not is_finite_y[t:t+horizon].all():\n",
    "            continue\n",
    "        if not is_finite_xe[t:t+horizon].all():\n",
    "            continue\n",
    "        t_list.append(t)\n",
    "    return np.array(t_list, dtype=int)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_one(model, y, xe, t, lookback, horizon, device):\n",
    "    x_power = torch.from_numpy(y[t-lookback:t][:, None]).float().unsqueeze(0).to(device)\n",
    "    x_exog  = torch.from_numpy(xe[t:t+horizon, :]).float().unsqueeze(0).to(device)\n",
    "    y_pred = model(x_power, x_exog).squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "    y_true = y[t:t+horizon].astype(np.float32)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def plot_one_day(run_name, day_start_ts, y_true_24, y_pred_24, save_path=None):\n",
    "    h = np.arange(len(y_true_24))\n",
    "    plt.figure()\n",
    "    plt.plot(h, y_true_24, label=\"True\")\n",
    "    plt.plot(h, y_pred_24, label=\"Pred\")\n",
    "    plt.title(f\"{run_name} | {day_start_ts} | midnight->24h\")\n",
    "    plt.xlabel(\"Hour ahead (0~23)\")\n",
    "    plt.ylabel(TARGET_COL)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=150)\n",
    "    if SHOW_PLOTS:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# ✅ Load ONLY latest run ckpts via LATEST_RUN.json\n",
    "# ============================================================\n",
    "if not os.path.exists(LATEST_JSON):\n",
    "    raise RuntimeError(f\"LATEST_RUN.json not found: {LATEST_JSON}\\n\"\n",
    "                       f\"먼저 '수정된 학습 코드'를 실행해서 LATEST_RUN.json을 생성하세요.\")\n",
    "\n",
    "with open(LATEST_JSON, \"r\") as f:\n",
    "    latest = json.load(f)\n",
    "\n",
    "exp_dir = latest.get(\"exp_dir\", None)\n",
    "ckpt_items = latest.get(\"ckpts\", [])\n",
    "\n",
    "if not exp_dir or not ckpt_items:\n",
    "    raise RuntimeError(f\"LATEST_RUN.json 내용이 비어있습니다. exp_dir={exp_dir}, ckpts={len(ckpt_items)}\")\n",
    "\n",
    "# only last ckpts\n",
    "last_ckpts = [c for c in ckpt_items if c.get(\"type\") == \"last\"]\n",
    "last_ckpts = sorted(last_ckpts, key=lambda x: int(x.get(\"wi\", 0)))\n",
    "\n",
    "if not last_ckpts:\n",
    "    raise RuntimeError(\"LATEST_RUN.json에 last ckpt가 없습니다. (학습 코드에서 last 저장 확인)\")\n",
    "\n",
    "print(f\"[INFO] Latest RUN_ID={latest.get('run_id')} | exp_dir={exp_dir}\")\n",
    "print(f\"[INFO] last ckpts in latest run: {len(last_ckpts)}\")\n",
    "\n",
    "# ============================================================\n",
    "# Run evaluation\n",
    "# ============================================================\n",
    "summary_rows = []\n",
    "\n",
    "for item in last_ckpts:\n",
    "    p = item[\"path\"]\n",
    "    wi = int(item.get(\"wi\", -1))\n",
    "\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"[WARN] ckpt path missing -> skip: {p}\")\n",
    "        continue\n",
    "\n",
    "    ckpt = torch.load(p, map_location=\"cpu\")\n",
    "    run_name = ckpt.get(\"run_name\", os.path.basename(p).replace(\".pt\", \"\"))\n",
    "\n",
    "    window = ckpt.get(\"window\", {})\n",
    "    cfg = ckpt.get(\"config\", {})\n",
    "\n",
    "    lookback = int(cfg.get(\"LOOKBACK\", 168))\n",
    "    horizon  = int(cfg.get(\"HORIZON\", 24))\n",
    "    exog_cols = cfg.get(\"EXOG_COLS\", None)\n",
    "    if exog_cols is None:\n",
    "        raise ValueError(f\"[{run_name}] ckpt에 EXOG_COLS가 없습니다.\")\n",
    "\n",
    "    val_start = pd.to_datetime(window.get(\"val_start\"))\n",
    "    val_end   = pd.to_datetime(window.get(\"val_end\"))\n",
    "\n",
    "    # ---- build arrays per this ckpt config\n",
    "    df_feat_local = df[[TARGET_COL] + list(exog_cols)].copy()\n",
    "    y_arr = df_feat_local[TARGET_COL].values.astype(np.float32)\n",
    "    xe_arr = df_feat_local[list(exog_cols)].values.astype(np.float32)\n",
    "    idx_dt = df_feat_local.index\n",
    "\n",
    "    # ---- midnight-only inside OWN val range\n",
    "    t_list = build_midnight_val_t_list(idx_dt, y_arr, xe_arr, lookback, horizon, val_start, val_end)\n",
    "\n",
    "    print(f\"\\n[w={wi:04d}] {run_name}\")\n",
    "    print(f\"  val_range={val_start} ~ {val_end}\")\n",
    "    if len(t_list) > 0:\n",
    "        print(f\"  midnight_samples={len(t_list)} (first={idx_dt[t_list[0]]}, last={idx_dt[t_list[-1]]})\")\n",
    "    else:\n",
    "        print(f\"  midnight_samples=0\")\n",
    "\n",
    "    if len(t_list) == 0:\n",
    "        summary_rows.append({\n",
    "            \"wi\": wi, \"run_name\": run_name, \"ckpt_path\": p,\n",
    "            \"val_start\": str(val_start), \"val_end\": str(val_end),\n",
    "            \"midnight_samples\": 0,\n",
    "            \"rmse\": np.nan, \"diff_rmse\": np.nan, \"cos_centered\": np.nan,\n",
    "            \"top_iou\": np.nan, \"share_overlap_pct\": np.nan, \"share_tv\": np.nan,\n",
    "            \"note\": \"no valid midnight samples\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # load model\n",
    "    model = LSTM24(exog_dim=len(exog_cols), horizon=horizon, hidden=32).to(DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    # infer\n",
    "    y_true_mat, y_pred_mat = [], []\n",
    "    for t in t_list:\n",
    "        yt, yp = infer_one(model, y_arr, xe_arr, t, lookback, horizon, DEVICE)\n",
    "        y_true_mat.append(yt)\n",
    "        y_pred_mat.append(yp)\n",
    "    y_true_mat = np.stack(y_true_mat, axis=0)\n",
    "    y_pred_mat = np.stack(y_pred_mat, axis=0)\n",
    "\n",
    "    # metrics\n",
    "    rmse = float(np.sqrt(np.mean((y_pred_mat - y_true_mat) ** 2)))\n",
    "    drmse = diff_rmse_np(y_pred_mat, y_true_mat)\n",
    "    cosc = cos_centered_np(y_pred_mat, y_true_mat)\n",
    "    tiou = topk_iou_np(y_pred_mat, y_true_mat, alpha=TOP_ALPHA)\n",
    "    shov = share_overlap_percent_np(y_pred_mat, y_true_mat)\n",
    "    shtv = share_tv_np(y_pred_mat, y_true_mat)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"wi\": wi, \"run_name\": run_name, \"ckpt_path\": p,\n",
    "        \"val_start\": str(val_start), \"val_end\": str(val_end),\n",
    "        \"midnight_samples\": int(len(t_list)),\n",
    "        \"rmse\": rmse, \"diff_rmse\": drmse, \"cos_centered\": cosc,\n",
    "        \"top_iou\": tiou, \"share_overlap_pct\": shov, \"share_tv\": shtv,\n",
    "        \"note\": \"\",\n",
    "    })\n",
    "\n",
    "    print(f\"  metrics: RMSE={rmse:.4f}, dRMSE={drmse:.4f}, cosC={cosc:.4f}, IoU={tiou:.4f}, shareOv={shov:.2f}%\")\n",
    "\n",
    "    # plots\n",
    "    take = min(MAX_DAYS_PER_MODEL, len(t_list))\n",
    "    for i in range(take):\n",
    "        day_ts = idx_dt[t_list[i]]\n",
    "        save_path = None\n",
    "        if SAVE_PLOTS:\n",
    "            safe = run_name.replace(\":\", \"\").replace(\" \", \"_\")\n",
    "            model_dir = os.path.join(PLOTS_DIR, safe)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            save_path = os.path.join(model_dir, f\"w{wi:04d}_day_{i:03d}_{day_ts.strftime('%Y%m%d')}.png\")\n",
    "        plot_one_day(run_name, day_ts, y_true_mat[i], y_pred_mat[i], save_path=save_path)\n",
    "\n",
    "# summary table inline\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"wi\"])\n",
    "display(summary_df)\n",
    "\n",
    "# save summary\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "out_csv = os.path.join(OUT_DIR, \"summary_midnight_last_latest.csv\")\n",
    "summary_df.to_csv(out_csv, index=False)\n",
    "print(f\"[INFO] saved summary -> {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
